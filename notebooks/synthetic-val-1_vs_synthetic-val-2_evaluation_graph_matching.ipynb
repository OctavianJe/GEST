{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27e5df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from gest.data.gest import GEST\n",
    "from gest.service.evaluation.graph_matching.graph import GESTGraph\n",
    "from gest.service.evaluation.graph_matching.similarity import (\n",
    "    SimilarityService,\n",
    "    SimilarityEngine,\n",
    ")\n",
    "from gest.service.evaluation.graph_matching.solver import SolverType\n",
    "from gest.service.evaluation.graph_matching.embedding_type_enum import EmbeddingType\n",
    "from gest.service.other.text_similarity.text_similarity_evaluator import (\n",
    "    TextSimilarityEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e68a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_CSV_PATH = \"/workspaces/GEST/notebooks/data/synthetic-val-1_vs_synthetic-val-2_evaluation_graph_matching.csv\"\n",
    "NEG_RESULTS_CSV_PATH = \"/workspaces/GEST/notebooks/data/synthetic-val-1_vs_synthetic-val-2_evaluation_graph_matching_negatives.csv\"\n",
    "TEXT_SIMILARITY_CSV_PATH = \"/workspaces/GEST/notebooks/data/synthetic-val-1_vs_synthetic-val-2_evaluation_text_similarity.csv\"\n",
    "\n",
    "REQUIRED_COLUMNS = {\"dataset\", \"id\", \"text\", \"gest\"}\n",
    "EVAL_NEG_PER_POS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6461a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = pd.read_csv(\"/workspaces/GEST/data/gest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a332a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_required_columns(df: pd.DataFrame, name: str, required_columns: set):\n",
    "    missing = required_columns - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "\n",
    "def ensure_duplicated_pairs(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    dup_mask = df.duplicated([\"dataset\", \"id\"], keep=False)\n",
    "    if not dup_mask.any():\n",
    "        raise ValueError(f\"{name} has no duplicated (dataset, id) pairs.\")\n",
    "    dups = df.loc[dup_mask, [\"dataset\", \"id\", \"text\", \"gest\"]].copy()\n",
    "\n",
    "    # Text uniqueness summary\n",
    "    per_key = dups.groupby([\"dataset\", \"id\"])[\"text\"].nunique()\n",
    "    same_text_keys = (per_key == 1).sum()\n",
    "    different_text_keys = (per_key > 1).sum()\n",
    "    print(\n",
    "        f\"{name}: {len(dups)} rows across {len(per_key)} duplicated keys \"\n",
    "        f\"(keys with identical text only: {same_text_keys}; keys with > 1 unique texts: {different_text_keys}).\"\n",
    "    )\n",
    "\n",
    "    # Keep only keys that actually have > 1 unique texts as we don't want identical-text pairs\n",
    "    dups = dups.merge(\n",
    "        per_key[per_key > 1].rename(\"n_unique_texts\").reset_index(),\n",
    "        on=[\"dataset\", \"id\"],\n",
    "        how=\"inner\",\n",
    "    ).drop(columns=\"n_unique_texts\")\n",
    "\n",
    "    if dups.empty:\n",
    "        raise ValueError(\n",
    "            \"All duplicated keys have identical text only; nothing to pair with different text.\"\n",
    "        )\n",
    "    return dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56210f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_required_columns(synthetic, \"synthetic\", REQUIRED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ff1897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic: 194 rows across 97 duplicated keys (keys with identical text only: 0; keys with > 1 unique texts: 97).\n"
     ]
    }
   ],
   "source": [
    "# Rows that belong to duplicated (dataset,id) keys with ≥2 unique texts\n",
    "dups = ensure_duplicated_pairs(synthetic, \"synthetic\")\n",
    "\n",
    "# Stable row ids to remove self and mirrored pairs later\n",
    "dups = dups.reset_index(names=\"row_id\")\n",
    "\n",
    "# Self-merge within key\n",
    "pairs = dups.merge(\n",
    "    dups,\n",
    "    on=[\"dataset\", \"id\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_val1\", \"_val2\"),\n",
    ")\n",
    "\n",
    "# Keep only unique unordered pairs (i < j)\n",
    "pairs = pairs[pairs[\"row_id_val1\"] < pairs[\"row_id_val2\"]].copy()\n",
    "\n",
    "# Drop pairs with identical text and identical gest JSON\n",
    "pairs = pairs[\n",
    "    (pairs[\"text_val1\"] != pairs[\"text_val2\"])\n",
    "    & (pairs[\"gest_val1\"] != pairs[\"gest_val2\"])\n",
    "].copy()\n",
    "\n",
    "# Tidy columns for downstream parsing\n",
    "pairs = pairs.rename(\n",
    "    columns={\n",
    "        \"gest_val1\": \"gest_synthetic_val1\",\n",
    "        \"gest_val2\": \"gest_synthetic_val2\",\n",
    "    }\n",
    ")[[\"dataset\", \"id\", \"gest_synthetic_val1\", \"gest_synthetic_val2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81846cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97 unique pairs across 97 duplicated (dataset, id) keys (with differing text).\n"
     ]
    }
   ],
   "source": [
    "num_keys = pairs[[\"dataset\", \"id\"]].drop_duplicates().shape[0]\n",
    "print(\n",
    "    f\"Found {len(pairs)} unique pairs across {num_keys} duplicated (dataset, id) keys (with differing text).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0630bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Synthetic Validation 1 GESTs: 100%|██████████| 97/97 [00:00<00:00, 534.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Parsing Synthetic Validation 1 GESTs\")\n",
    "pairs[\"g1\"] = pairs[\"gest_synthetic_val1\"].progress_apply(\n",
    "    lambda s: GESTGraph(gest=GEST.model_validate_json(s))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c3b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Synthetic Validation 2 GESTs: 100%|██████████| 97/97 [00:00<00:00, 2218.53it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Parsing Synthetic Validation 2 GESTs\")\n",
    "pairs[\"g2\"] = pairs[\"gest_synthetic_val2\"].progress_apply(\n",
    "    lambda s: GESTGraph(gest=GEST.model_validate_json(s))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe78e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [\n",
    "    {\n",
    "        \"name\": \"Spectral_GloVe50\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE50,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NGM_GloVe50\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.NGM,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE50,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spectral_GloVe50_NoEdges\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE50,\n",
    "            \"use_edges\": False,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spectral_GloVe300\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE300,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NGM_GloVe300\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.NGM,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE300,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spectral_GloVe300_NoEdges\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.GLOVE300,\n",
    "            \"use_edges\": False,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spectral_W2V300\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.W2V_GOOGLE,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NGM_W2V300\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.NGM,\n",
    "            \"embedding_type\": EmbeddingType.W2V_GOOGLE,\n",
    "            \"use_edges\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Spectral_W2V300_NoEdges\",\n",
    "        \"engine_params\": {\n",
    "            \"solver_type\": SolverType.SPECTRAL,\n",
    "            \"embedding_type\": EmbeddingType.W2V_GOOGLE,\n",
    "            \"use_edges\": False,\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638112fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Evaluation for 'Spectral_GloVe50'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_GloVe50' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'NGM_GloVe50'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'NGM_GloVe50' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe50_NoEdges'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_GloVe50_NoEdges' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe300'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_GloVe300' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'NGM_GloVe300'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'NGM_GloVe300' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe300_NoEdges'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_GloVe300_NoEdges' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'Spectral_W2V300'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_W2V300' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'NGM_W2V300'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'NGM_W2V300' are already processed. Skipping.\n",
      "\n",
      "Starting Evaluation for 'Spectral_W2V300_NoEdges'.\n",
      "Found 97 previously computed results for this configuration.\n",
      "All pairs for 'Spectral_W2V300_NoEdges' are already processed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "for config in configurations:\n",
    "    config_name = config[\"name\"]\n",
    "    print(f\"\\nStarting Evaluation for '{config_name}'.\")\n",
    "\n",
    "    processed_pairs = set()\n",
    "    if os.path.exists(RESULTS_CSV_PATH):\n",
    "        temp_df = pd.read_csv(RESULTS_CSV_PATH)\n",
    "        processed_for_config = temp_df[temp_df[\"configuration\"] == config_name]\n",
    "        processed_pairs = set(\n",
    "            zip(processed_for_config[\"dataset\"], processed_for_config[\"id\"])\n",
    "        )\n",
    "\n",
    "    if processed_pairs:\n",
    "        print(\n",
    "            f\"Found {len(processed_pairs)} previously computed results for this configuration.\"\n",
    "        )\n",
    "\n",
    "    pairs[\"is_processed\"] = [\n",
    "        (d, i) in processed_pairs for d, i in zip(pairs[\"dataset\"], pairs[\"id\"])\n",
    "    ]\n",
    "    pairs_to_process = pairs[~pairs[\"is_processed\"]].copy()\n",
    "\n",
    "    if pairs_to_process.empty:\n",
    "        print(f\"All pairs for '{config_name}' are already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {len(pairs_to_process)} new pairs for '{config_name}'.\")\n",
    "    engine = SimilarityEngine(**config[\"engine_params\"])\n",
    "    similarity_service = SimilarityService(engine=engine)\n",
    "\n",
    "    def compute_similarity(row) -> float:\n",
    "        try:\n",
    "            return similarity_service.graph_similarity_normalized(row[\"g1\"], row[\"g2\"])\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Exception occurred on (dataset={row['dataset']}, id={row['id']}): \\n{e}\"\n",
    "            )\n",
    "            return 0.0\n",
    "\n",
    "    csv_header = [\"dataset\", \"id\", \"configuration\", \"similarity\"]\n",
    "    write_header = not os.path.exists(RESULTS_CSV_PATH)\n",
    "\n",
    "    with open(RESULTS_CSV_PATH, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=csv_header)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for _, row in tqdm(\n",
    "            pairs_to_process.iterrows(),\n",
    "            total=len(pairs_to_process),\n",
    "            desc=f\"Calculating for {config_name}\",\n",
    "        ):\n",
    "            score = compute_similarity(row)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"dataset\": row[\"dataset\"],\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"configuration\": config_name,\n",
    "                    \"similarity\": score,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80e6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_score(pos_vals: pd.Series, neg_vals: pd.Series) -> float:\n",
    "    mu1, mu0 = pos_vals.mean(), neg_vals.mean()\n",
    "    v1, v0 = pos_vals.var(ddof=1), neg_vals.var(ddof=1)\n",
    "    return float(((mu1 - mu0) ** 2) / (v1 + v0 + 1e-12))\n",
    "\n",
    "\n",
    "def pr_auc(scores: pd.Series, labels: pd.Series) -> float:\n",
    "    return float(average_precision_score(labels, scores))\n",
    "\n",
    "\n",
    "def top1_accuracy(df: pd.DataFrame, score_col: str) -> float:\n",
    "    hits = []\n",
    "    for _, g in df.groupby([\"dataset\", \"id\"]):\n",
    "        if g[\"label\"].sum() == len(g):\n",
    "            continue\n",
    "        g = g.sort_values(score_col, ascending=False)\n",
    "        hits.append(int(g.iloc[0][\"label\"] == 1))\n",
    "    return float(np.mean(hits)) if hits else float(\"nan\")\n",
    "\n",
    "\n",
    "def point_biserial_corr(scores: pd.Series, labels: pd.Series) -> float:\n",
    "    s = scores.to_numpy(dtype=float)\n",
    "    y = labels.to_numpy(dtype=float)\n",
    "    if s.std() < 1e-12 or y.std() < 1e-12:\n",
    "        return float(\"nan\")\n",
    "    return float(np.corrcoef(s, y)[0, 1])\n",
    "\n",
    "\n",
    "def _stable_rng(dataset: str, ex_id: str, seed: int = 0) -> np.random.Generator:\n",
    "    h = hashlib.sha256(f\"{dataset}::{ex_id}::{seed}\".encode()).digest()\n",
    "    return np.random.default_rng(int.from_bytes(h[:4], \"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2163916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _neg_pairs_for_keys(\n",
    "    pairs_df: pd.DataFrame, subset_keys, neg_per_pos: int\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    by_ds = {ds: grp.sort_values(\"id\") for ds, grp in pairs_df.groupby(\"dataset\")}\n",
    "    for ds, ex_id in subset_keys:\n",
    "        ex_id = str(ex_id)\n",
    "        grp = by_ds.get(ds)\n",
    "        if grp is None or len(grp) < 2:\n",
    "            continue\n",
    "        candidates = [str(i) for i in grp[\"id\"].tolist() if str(i) != ex_id]\n",
    "        if not candidates:\n",
    "            continue\n",
    "        rng = _stable_rng(ds, ex_id)\n",
    "        k = min(neg_per_pos, len(candidates))\n",
    "        choose = rng.choice(candidates, size=k, replace=len(candidates) < k)\n",
    "        for neg_id in np.atleast_1d(choose):\n",
    "            rows.append({\"dataset\": ds, \"id\": ex_id, \"neg_id\": str(neg_id)})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e943a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_all = pairs[[\"dataset\", \"id\", \"g1\", \"g2\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab055868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_negative_scores_cached(\n",
    "    config_name: str, engine_params: dict, keys\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Cache negative scores for desired (dataset,id) keys and return the subset.\"\"\"\n",
    "    if os.path.exists(NEG_RESULTS_CSV_PATH):\n",
    "        neg_cache = pd.read_csv(NEG_RESULTS_CSV_PATH)\n",
    "        neg_cache[\"id\"] = neg_cache[\"id\"].astype(str)\n",
    "        neg_cache[\"neg_id\"] = neg_cache[\"neg_id\"].astype(str)\n",
    "    else:\n",
    "        neg_cache = pd.DataFrame(\n",
    "            columns=[\"dataset\", \"id\", \"neg_id\", \"configuration\", \"similarity\"]\n",
    "        )\n",
    "\n",
    "    have = (\n",
    "        neg_cache[neg_cache[\"configuration\"] == config_name]\n",
    "        if len(neg_cache)\n",
    "        else neg_cache\n",
    "    )\n",
    "    have_keys = set(zip(have[\"dataset\"], have[\"id\"], have[\"neg_id\"]))\n",
    "\n",
    "    desired_pairs = _neg_pairs_for_keys(pairs_all, keys, EVAL_NEG_PER_POS)\n",
    "    if desired_pairs.empty:\n",
    "        return have\n",
    "\n",
    "    desired_keys = set(\n",
    "        zip(desired_pairs[\"dataset\"], desired_pairs[\"id\"], desired_pairs[\"neg_id\"])\n",
    "    )\n",
    "    to_compute_keys = desired_keys - have_keys\n",
    "\n",
    "    if to_compute_keys:\n",
    "        g1_map = {\n",
    "            (d, str(i)): g\n",
    "            for d, i, g in pairs_all[[\"dataset\", \"id\", \"g1\"]].itertuples(index=False)\n",
    "        }\n",
    "        g2_map = {\n",
    "            (d, str(i)): g\n",
    "            for d, i, g in pairs_all[[\"dataset\", \"id\", \"g2\"]].itertuples(index=False)\n",
    "        }\n",
    "\n",
    "        engine = SimilarityEngine(**engine_params)\n",
    "        sim = SimilarityService(engine=engine)\n",
    "\n",
    "        print(\n",
    "            f\"Processing {len(to_compute_keys)} new negative pairs for '{config_name}'.\"\n",
    "        )\n",
    "        rows = []\n",
    "        for ds, ex_id, neg_id in tqdm(\n",
    "            list(to_compute_keys),\n",
    "            total=len(to_compute_keys),\n",
    "            desc=f\"Negatives for {config_name}\",\n",
    "        ):\n",
    "            try:\n",
    "                g1 = g1_map[(ds, str(ex_id))]\n",
    "                g2 = g2_map[(ds, str(neg_id))]\n",
    "                score = sim.graph_similarity_normalized(g1, g2)\n",
    "            except Exception:\n",
    "                score = 0.0\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"dataset\": ds,\n",
    "                    \"id\": str(ex_id),\n",
    "                    \"neg_id\": str(neg_id),\n",
    "                    \"configuration\": config_name,\n",
    "                    \"similarity\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        write_header = not os.path.exists(NEG_RESULTS_CSV_PATH)\n",
    "        with open(NEG_RESULTS_CSV_PATH, \"a\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(\n",
    "                f, fieldnames=[\"dataset\", \"id\", \"neg_id\", \"configuration\", \"similarity\"]\n",
    "            )\n",
    "            if write_header:\n",
    "                w.writeheader()\n",
    "            for r in rows:\n",
    "                w.writerow(r)\n",
    "\n",
    "        neg_cache = pd.read_csv(NEG_RESULTS_CSV_PATH)\n",
    "        neg_cache[\"id\"] = neg_cache[\"id\"].astype(str)\n",
    "        neg_cache[\"neg_id\"] = neg_cache[\"neg_id\"].astype(str)\n",
    "\n",
    "    neg_sub = neg_cache[neg_cache[\"configuration\"] == config_name]\n",
    "    return neg_sub.merge(desired_pairs, on=[\"dataset\", \"id\", \"neg_id\"], how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdc4fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Evaluation for 'Spectral_GloVe50' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'NGM_GloVe50' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe50_NoEdges' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe300' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'NGM_GloVe300' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'Spectral_GloVe300_NoEdges' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'Spectral_W2V300' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'NGM_W2V300' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n",
      "\n",
      "Starting Evaluation for 'Spectral_W2V300_NoEdges' (negatives & metrics).\n",
      "Found 388 previously computed negative results for this configuration.\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(RESULTS_CSV_PATH)\n",
    "summary_rows = []\n",
    "\n",
    "for config in configurations:\n",
    "    name = config[\"name\"]\n",
    "\n",
    "    pos_scores_df = results_df[results_df[\"configuration\"] == name][\n",
    "        [\"dataset\", \"id\", \"similarity\"]\n",
    "    ].copy()\n",
    "    if pos_scores_df.empty:\n",
    "        print(f\"\\nStarting Evaluation for '{name}'.\")\n",
    "        print(\"Found 0 previously computed results for this configuration.\")\n",
    "        print(f\"All pairs for '{name}' are already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nStarting Evaluation for '{name}' (negatives & metrics).\")\n",
    "    keys = set(zip(pos_scores_df[\"dataset\"], pos_scores_df[\"id\"].astype(str)))\n",
    "\n",
    "    existing = (\n",
    "        pd.read_csv(NEG_RESULTS_CSV_PATH)\n",
    "        if os.path.exists(NEG_RESULTS_CSV_PATH)\n",
    "        else pd.DataFrame(\n",
    "            columns=[\"dataset\", \"id\", \"neg_id\", \"configuration\", \"similarity\"]\n",
    "        )\n",
    "    )\n",
    "    existing = existing[(existing[\"configuration\"] == name)]\n",
    "    existing[\"id\"] = existing[\"id\"].astype(str)\n",
    "    existing[\"neg_id\"] = existing[\"neg_id\"].astype(str)\n",
    "    already = existing.merge(\n",
    "        _neg_pairs_for_keys(pairs_all, keys, EVAL_NEG_PER_POS),\n",
    "        on=[\"dataset\", \"id\", \"neg_id\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    print(\n",
    "        f\"Found {len(already)} previously computed negative results for this configuration.\"\n",
    "    )\n",
    "\n",
    "    neg_cached = _ensure_negative_scores_cached(name, config[\"engine_params\"], keys)\n",
    "\n",
    "    pos_scores_df = pos_scores_df.rename(columns={\"similarity\": \"score\"}).copy()\n",
    "    pos_scores_df[\"id\"] = pos_scores_df[\"id\"].astype(str)\n",
    "    pos_scores_df[\"label\"] = 1\n",
    "\n",
    "    neg_scores_df = (\n",
    "        neg_cached[[\"dataset\", \"id\", \"neg_id\", \"similarity\"]]\n",
    "        .rename(columns={\"similarity\": \"score\"})\n",
    "        .copy()\n",
    "    )\n",
    "    neg_scores_df[\"id\"] = neg_scores_df[\"id\"].astype(str)\n",
    "    neg_scores_df[\"label\"] = 0\n",
    "\n",
    "    eval_df = pd.concat(\n",
    "        [\n",
    "            pos_scores_df[[\"dataset\", \"id\", \"label\", \"score\"]],\n",
    "            neg_scores_df[[\"dataset\", \"id\", \"label\", \"score\"]],\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    corr = 100.0 * point_biserial_corr(eval_df[\"score\"], eval_df[\"label\"])\n",
    "    acc = 100.0 * top1_accuracy(eval_df, \"score\")\n",
    "    fsc = fisher_score(pos_scores_df[\"score\"], neg_scores_df[\"score\"])\n",
    "    auc = 100.0 * pr_auc(eval_df[\"score\"], eval_df[\"label\"])\n",
    "\n",
    "    summary_rows.append(\n",
    "        {\"Configuration\": name, \"Corr\": corr, \"Acc\": acc, \"F\": fsc, \"AUC\": auc}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "372a68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics for each experiment:\n",
      "------------------------------------\n",
      "                             Corr     Acc      F     AUC\n",
      "Configuration                                           \n",
      "NGM_GloVe300               26.098  41.237  0.156  38.240\n",
      "NGM_GloVe50                16.916  29.897  0.077  30.323\n",
      "NGM_W2V300                  9.945  23.711  0.031  24.235\n",
      "Spectral_GloVe300          24.770  43.299  0.188  34.724\n",
      "Spectral_GloVe300_NoEdges  38.293  58.763  0.506  49.409\n",
      "Spectral_GloVe50           16.115  32.990  0.082  29.936\n",
      "Spectral_GloVe50_NoEdges   28.369  51.546  0.269  43.011\n",
      "Spectral_W2V300             9.688  28.866  0.029  24.768\n",
      "Spectral_W2V300_NoEdges     6.983  55.670  0.018  22.876\n"
     ]
    }
   ],
   "source": [
    "eval_summary = (\n",
    "    pd.DataFrame(summary_rows).sort_values(\"Configuration\").set_index(\"Configuration\")\n",
    ")\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "print(\"\\nEvaluation metrics for each experiment:\")\n",
    "print(\"------------------------------------\")\n",
    "print(eval_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72a4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 10:00:52,206 sentence_transformers.SentenceTransformer [INFO] Use pytorch device_name: cpu\n",
      "2025-08-26 10:00:52,207 sentence_transformers.SentenceTransformer [INFO] Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "def compute_text_similarity_for_pairs(\n",
    "    pairs_df: pd.DataFrame,\n",
    "    dups_df: pd.DataFrame,\n",
    "    out_csv_path: str = TEXT_SIMILARITY_CSV_PATH,\n",
    "    evaluator=TextSimilarityEvaluator(),\n",
    "):\n",
    "    left_text = dups_df.rename(\n",
    "        columns={\"gest\": \"gest_synthetic_val1\", \"text\": \"text_val1\"}\n",
    "    )[[\"dataset\", \"id\", \"gest_synthetic_val1\", \"text_val1\"]]\n",
    "    right_text = dups_df.rename(\n",
    "        columns={\"gest\": \"gest_synthetic_val2\", \"text\": \"text_val2\"}\n",
    "    )[[\"dataset\", \"id\", \"gest_synthetic_val2\", \"text_val2\"]]\n",
    "\n",
    "    pairs_with_text = (\n",
    "        pairs_df.merge(\n",
    "            left_text, on=[\"dataset\", \"id\", \"gest_synthetic_val1\"], how=\"left\"\n",
    "        )\n",
    "        .merge(right_text, on=[\"dataset\", \"id\", \"gest_synthetic_val2\"], how=\"left\")\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    sims = []\n",
    "    for t1, t2 in tqdm(\n",
    "        zip(pairs_with_text[\"text_val1\"], pairs_with_text[\"text_val2\"]),\n",
    "        total=len(pairs_with_text),\n",
    "        desc=\"Text similarity\",\n",
    "    ):\n",
    "        sims.append(float(evaluator.compute_text_similarity(str(t1), str(t2))))\n",
    "\n",
    "    pairs_with_text[\"text_similarity\"] = sims\n",
    "    mean_score = float(np.mean(sims)) if sims else float(\"nan\")\n",
    "\n",
    "    cols_to_save = [\"dataset\", \"id\", \"text_val1\", \"text_val2\", \"text_similarity\"]\n",
    "    pairs_with_text[cols_to_save].to_csv(out_csv_path, index=False)\n",
    "\n",
    "    print(\"\\nText similarity evaluation:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\n",
    "        f\"Computed text similarity for {len(pairs_with_text)} pairs; mean = {mean_score:.6f}\"\n",
    "    )\n",
    "    print(f\"Saved pairwise text similarity to: {out_csv_path}\")\n",
    "\n",
    "    return pairs_with_text, mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0beecc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]2.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]4.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]5.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]5.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]7.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]7.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.17it/s]7.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]8.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s] 8.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s] 7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s] 8.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s] 8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s] 7.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s] 6.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s] 6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s] 5.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s] 5.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s] 3.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s] 4.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s] 4.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s] 4.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s] 5.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.06it/s] 5.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s] 6.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s] 6.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s] 6.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s] 7.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s] 7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s] 5.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s] 6.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s] 6.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s] 7.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s] 7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.10it/s] 7.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.37it/s] 7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s] 6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s] 6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s] 6.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s] 6.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.42it/s] 8.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s] 8.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s] 8.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s] 8.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s] 8.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s] 8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.31it/s] 8.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s] 9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s] 8.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s] 7.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s] 7.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s] 7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s] 7.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s] 7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s] 7.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s] 7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.62it/s] 7.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s] 7.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.70it/s] 7.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s] 7.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s] 7.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s] 7.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s] 4.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s] 5.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s] 5.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s] 6.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s] 6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s] 6.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.75it/s] 6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s] 7.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s] 7.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s] 7.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s] 7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s] 7.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.84it/s] 7.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.12it/s] 8.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s] 8.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s] 8.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.44it/s] 8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s] 9.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.31it/s] 9.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.74it/s]10.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]10.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]10.46it/s]\n",
      "Text similarity: 100%|██████████| 97/97 [00:13<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text similarity evaluation:\n",
      "------------------------------------\n",
      "Computed text similarity for 97 pairs; mean = 0.735890\n",
      "Saved pairwise text similarity to: /workspaces/GEST/notebooks/data/synthetic-val-1_vs_synthetic-val-2_evaluation_text_similarity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                 dataset             id  \\\n",
       " 0   ActivityNet Captions  v_IQGg87yZZjs   \n",
       " 1   ActivityNet Captions  v_ZJ6BFrKcRe0   \n",
       " 2   ActivityNet Captions  v_J3DxJ8gI95U   \n",
       " 3   ActivityNet Captions  v_KRGiJIHSd9E   \n",
       " 4   ActivityNet Captions  v_Iq9cAZxki9Y   \n",
       " ..                   ...            ...   \n",
       " 92  ActivityNet Captions  v_UgSLUt8X1Lc   \n",
       " 93  ActivityNet Captions  v_fgoXpih2Kws   \n",
       " 94  ActivityNet Captions  v_tznMNEWglxY   \n",
       " 95  ActivityNet Captions  v_bmoS216hsoc   \n",
       " 96  ActivityNet Captions  v_fOuFF7dGPtI   \n",
       " \n",
       "                                   gest_synthetic_val1  \\\n",
       " 0   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 1   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 2   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 3   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 4   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " ..                                                ...   \n",
       " 92  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 93  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 94  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 95  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 96  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " \n",
       "                                   gest_synthetic_val2  \\\n",
       " 0   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 1   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 2   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 3   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 4   {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " ..                                                ...   \n",
       " 92  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 93  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 94  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 95  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " 96  {\"actor0\":{\"action\":\"Exists\",\"entities\":[\"acto...   \n",
       " \n",
       "                                                    g1  \\\n",
       " 0   gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 1   gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 2   gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 3   gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 4   gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " ..                                                ...   \n",
       " 92  gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 93  gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 94  gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 95  gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " 96  gest=GEST(actors={'actor0': Actor(action='Exis...   \n",
       " \n",
       "                                                    g2  is_processed  \\\n",
       " 0   gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 1   gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 2   gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 3   gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 4   gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " ..                                                ...           ...   \n",
       " 92  gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 93  gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 94  gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 95  gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " 96  gest=GEST(actors={'actor0': Actor(action='Exis...          True   \n",
       " \n",
       "                                             text_val1  \\\n",
       " 0   A hand is shown hovering over assorted cutlery...   \n",
       " 1   The man in on the beach is flying the kite. Th...   \n",
       " 2   A stove is shown with a skillet on an eye with...   \n",
       " 3   A female tattoo artist is doing a tattoo back ...   \n",
       " 4   Two people are sitting at a table. They are pl...   \n",
       " ..                                                ...   \n",
       " 92  A woman is sweeping the floor in a kitchen. Sh...   \n",
       " 93  Two young boys play fight in a room with a bla...   \n",
       " 94  A woman in a blue shirt is standing next to a ...   \n",
       " 95  A girl in a black shirt is sitting down. She b...   \n",
       " 96  A man demonstrates how to bowl a ball. He show...   \n",
       " \n",
       "                                             text_val2  text_similarity  \n",
       " 0   various knives are shown close up. a boy picks...            0.486  \n",
       " 1   The man in gray shirt is standing at the beach...            0.831  \n",
       " 2   a chef is working inside a kitchen. He is cook...            0.667  \n",
       " 3   A woman is seen bending down and peeling paper...            0.749  \n",
       " 4   A woman gives the word for rock in Korean. A w...            0.465  \n",
       " ..                                                ...              ...  \n",
       " 92  A woman is sweeping with a broom in the kitche...            0.873  \n",
       " 93  a boy is standing in an empty room. another bo...            0.771  \n",
       " 94  Several girls are in a gymnastics gym practici...            0.849  \n",
       " 95  A woman is seen speaking to the camera holding...            0.556  \n",
       " 96  A man is standing up holding a bowling ball. H...            0.724  \n",
       " \n",
       " [97 rows x 10 columns],\n",
       " 0.7358902360975128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_text_similarity_for_pairs(pairs, dups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
